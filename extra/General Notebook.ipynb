{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polyphonic Music \n",
    "Below are the source files (MIDI) for the 4 datasets evaluated in the paper (split in train, validation and test sets). You can generate piano-rolls from the source files by transposing each sequence in C major or C minor and sampling frames every eighth note (quarter note for JSB chorales) following the beat information present in the MIDI file. Alternatively, pickled piano-rolls for use with the Python language are also provided\n",
    "\n",
    "Dataset [here](http://www-etud.iro.umontreal.ca/~boulanni/icml2012)\n",
    "\n",
    "This will load a dictionary with 'train', 'valid' and 'test' keys, with the corresponding values being a list of sequences. Each sequence is itself a list of time steps, and each time step is a list of the non-zero elements in the piano-roll at this instant (in MIDI note numbers, between 21 and 108 inclusive). \n",
    "\n",
    "\n",
    "MIDI notes are numbered from 0 to 127 assigned to C-1 to G9. This corresponds to a range of 8.175799 to 12543.85 Hz (assuming equal temperament and 440 Hz A4) and extends beyond the 88 note piano range from A0 to C8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from scipy.special import logsumexp\n",
    "from torchsummary import summary\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\kmorales\\\\Google Drive\\\\PhD\\\\Music_data' \n",
    "\n",
    "path = path + '\\\\jsb_raw_batchsize16_seqlen100.npz'\n",
    "data_set = np.load(path)\n",
    "\n",
    "print(data_set.files)\n",
    "print(data_set['x_train'].shape)\n",
    "print(data_set['x_valid'].shape)\n",
    "print(data_set['x_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(train,test,batch):\n",
    "    train_loader = torch.utils.data.DataLoader(train,batch_size=batch,shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test,batch_size=batch,shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 16\n",
    "train_loader, test_loader = analysis(data_set['x_train'], data_set['x_valid'], batch_sz)\n",
    "test_loader_elbo =  torch.utils.data.DataLoader(data_set['x_test'],batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(train_loader):\n",
    "    data = data.float()\n",
    "    data = data.transpose(0, 1)\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from vrnn_mnist_ import VRNN\n",
    "from vrnn2_mnist_ import VRNN\n",
    "from pcm2_mnist_ import PCM2\n",
    "from hmm2_mnist_ import HMM\n",
    "from pcm_c_mnist import PCM_c\n",
    "from pcm_c2_mnist import PCM_c2\n",
    "#from pcm_c3_mnist import PCM_c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/clinicalml/structuredinference/blob/master/parse_args_dkf.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directory\n",
    "\n",
    "folder = 'jsb/' # muse, notting , piano \n",
    "\n",
    "#--------------------------------------------\n",
    "img = True\n",
    "if img:\n",
    "    x_dim = 88\n",
    "else:\n",
    "    x_dim = 1\n",
    "\n",
    "h_dim_vrnn = 300\n",
    "\n",
    "h_dim = h_dim_vrnn\n",
    "\n",
    "\n",
    "z_dim = 300\n",
    "\n",
    "px_dim = 100\n",
    "\n",
    "pz_dim = 100\n",
    "\n",
    "qz_dim = 500 \n",
    "\n",
    "\n",
    "\n",
    "n_layers =  1\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "clip = 10\n",
    "\n",
    "learning_rate = 0.003#0.001\n",
    "\n",
    "\n",
    "print_every = 100\n",
    "\n",
    "save_every = 2\n",
    "\n",
    "#--------------------------------------------\n",
    "#Intializing empty arrays to store train and test NLL\n",
    "#manual seed\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "#--------------------------------------------\n",
    "# Model\n",
    "model = VRNN(x_dim, h_dim, z_dim, px_dim, pz_dim, qz_dim, n_layers)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay= 1e-4)\n",
    "#--------------------------------------------\n",
    "\n",
    "model_name = 'vrnn_state_'\n",
    "path_save = folder+model_name\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_LOSS_1, test_LOSS_1= run_model(train_loader,test_loader,model,optimizer,batch_sz,clip, path_save, n_epochs ,print_every, img, save_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch_model = 199\n",
    "# learning_rate = 0.005\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay= 1e-4)\n",
    "\n",
    "\n",
    "path = path_save+str(epoch_model)+'.pth'\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(loss)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_LOSS_1, test_LOSS_1,_,_,_,_= run_model(train_loader,test_loader,model,optimizer,batch_sz,clip, path_save, n_epochs ,train_NLL, train_KLD, train_LOSS, test_NLL, test_KLD, test_LOSS,print_every, img, save_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elb, lpx = ELBO(test_loader_elbo, model2, n_samples,folder, model_name)\n",
    "print(np.mean(elb))\n",
    "print(np.mean(lpx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'PCM'\n",
    "number_parameters(name_model, h_dim_vrnn, pz_dim, px_dim, qz_dim ,x_dim, z_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H dim\n",
    "h_dim = 84\n",
    "\n",
    "# # Model\n",
    "#--------------------------------------------\n",
    "\n",
    "model2 = PCM2(x_dim, h_dim, z_dim,px_dim, pz_dim, qz_dim, n_layers)\n",
    "\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate, weight_decay= 1e-4)\n",
    "#--------------------------------------------\n",
    "\n",
    "model_name = 'pcm_state_'\n",
    "path_save = folder+model_name\n",
    "#--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_LOSS_2, test_LOSS_2= run_model(train_loader,test_loader,model2,optimizer2,batch_sz,clip, path_save, n_epochs,print_every, img, save_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch_model = 199\n",
    "# learning_rate = 0.005\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay= 1e-4)\n",
    "\n",
    "\n",
    "path = path_save+str(epoch_model)+'.pth'\n",
    "checkpoint = torch.load(path)\n",
    "model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer2.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(loss)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elb2, lpx2 = ELBO(test_loader_elbo, model2, n_samples,folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(elb2))\n",
    "print(np.mean(lpx2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCM_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'PCM_c'\n",
    "number_parameters(name_model, h_dim_vrnn, pz_dim, px_dim, qz_dim ,x_dim, z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H dim\n",
    "\n",
    "h_dim = 71\n",
    "\n",
    "# Model\n",
    "#--------------------------------------------\n",
    "\n",
    "model3 = PCM_c(x_dim, h_dim, z_dim,px_dim, pz_dim, qz_dim, n_layers)\n",
    "\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr=learning_rate, weight_decay= 1e-4)\n",
    "#--------------------------------------------\n",
    "model_name = 'pcm_c_state_'\n",
    "path_save = folder+model_name\n",
    "#--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LOSS_3, test_LOSS_3 = run_model(train_loader,test_loader,model3,optimizer3,batch_sz,clip, path_save, n_epochs ,print_every, img, save_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_model = 199\n",
    "# learning_rate = 0.005\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay= 1e-4)\n",
    "\n",
    "\n",
    "path = path_save+str(epoch_model)+'.pth'\n",
    "checkpoint = torch.load(path)\n",
    "model3.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer3.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elb3, lpx3 = ELBO(test_loader_elbo, model3, n_samples,folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(elb4))\n",
    "print(np.mean(lpx4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCM_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'PMC_c2'\n",
    "number_parameters(name_model, h_dim_vrnn, pz_dim, px_dim, qz_dim ,x_dim, z_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H dim\n",
    "\n",
    "h_dim = 71\n",
    "\n",
    "# Model\n",
    "#--------------------------------------------\n",
    "model4 = PCM_c2(x_dim, h_dim, z_dim,px_dim, pz_dim, qz_dim, n_layers)\n",
    "\n",
    "optimizer4 = torch.optim.Adam(model4.parameters(), lr=learning_rate, weight_decay= 1e-4)\n",
    "#--------------------------------------------\n",
    "model_name = 'pcm_c2_state_'\n",
    "path_save = folder+model_name\n",
    "#--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_LOSS_4, test_LOSS_4 = run_model(train_loader,test_loader,model4,optimizer4,batch_sz,clip, path_save, n_epochs, print_every, img, save_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_model = 199\n",
    "# learning_rate = 0.005\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay= 1e-4)\n",
    "\n",
    "\n",
    "path = path_save+str(epoch_model)+'.pth'\n",
    "checkpoint = torch.load(path)\n",
    "model4.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer4.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elb4, lpx4 = ELBO(test_loader_elbo, model4, n_samples,folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(elb5))\n",
    "print(np.mean(lpx5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
